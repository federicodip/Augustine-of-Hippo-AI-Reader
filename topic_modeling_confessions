import pandas as pd

data = pd.read_csv("output_confessions_all.csv")
data.info()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# Ensure necessary resources are available
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Add custom stop words
custom_stop_words = list(ENGLISH_STOP_WORDS) + ['o', 'u', 'wa', 'art', ]

print(len(custom_stop_words))

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Custom tokenizer function with lemmatization
def custom_tokenizer(text):
    tokens = word_tokenize(text.lower())  # Tokenize and lowercase
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatize and remove non-alpha tokens
    return [token for token in lemmatized_tokens if token not in custom_stop_words]  # Remove stopwords

# Configure TF-IDF with custom tokenizer and stop words
tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, min_df=8, max_df=0.6)

# Tokenize and perform the TF-IDF calculation
doc_term_matrix = tfidf_vectorizer.fit_transform(data['Paragraph'])

# Check out the dimensions
print(f'Rows: {doc_term_matrix.shape[0]}, Columns: {doc_term_matrix.shape[1]}')

from sklearn.decomposition import LatentDirichletAllocation

# Learn X topics from the text documents
num_topics = 5

lda_topic_model = LatentDirichletAllocation(n_components = num_topics, random_state = 12345)

# Train the LDA model and get the document topic assignments
doc_topic_matrix = lda_topic_model.fit_transform(doc_term_matrix)

# I like pandas DataFrames
col_names = [f'Topic {x}' for x in range(1, num_topics + 1)]

# Display each document's topic assignments
doc_topic_df = pd.DataFrame(doc_topic_matrix, columns = col_names)
doc_topic_df.head(n = 10)

# Display the top X words for each topic
num_words = 6

for topic, words in enumerate(lda_topic_model.components_):
    word_total = words.sum()              # Get the total word weight for topic
    sorted_words = words.argsort()[::-1]  # Sort in descending order
    print(f'\nTopic {topic + 1:02d}')     # Print the topic
    for i in range(0, num_words):         # Print topic's top 10 words
        word = tfidf_vectorizer.get_feature_names_out()[sorted_words[i]]
        word_weight = words[sorted_words[i]] 
        print(f'  {word} ({word_weight:.3f})')



import matplotlib.pyplot as plt
import numpy as np

def plot_top_words(model, feature_names, n_top_words, title):
    fig, axes = plt.subplots(2, 3, figsize=(20,10))
    axes = axes.flatten()
    
    for topic_idx, topic in enumerate(model.components_):
        top_words_idx = topic.argsort()[:-n_top_words-1:-1]
        top_words = [feature_names[i] for i in top_words_idx]
        weights = topic[top_words_idx]
        
        ax = axes[topic_idx]
        ax.barh(top_words, weights)
        ax.set_title(f'Topic {topic_idx+1}')
        ax.invert_yaxis()
    
    plt.tight_layout()
    plt.savefig('topic_visualization.png')
    plt.show()

# Visualize the topics
plot_top_words(lda_topic_model, 
               tfidf_vectorizer.get_feature_names_out(),
               n_top_words=10,
               title='Topics in LDA model')
